---
layout: post
title:  "The ML Pricing Model That Didn't Work"
image: /assets/2023-05-21/airflow.png
description: How to use math in machine learning.
date: 2023-06-04
---

## Introduction

Among my many tidbits of advice for data professionals, and in this case data scientists, one of them is to **do actual math as part of your modeling and research process**.

Many data scientists think they are doing math when they are actually not. It feels math-y to import pre-made algorithms, create plots, and look at an AUC or F1 score. But that's not doing math.

I have a very hard time explaining what I mean by "do actual math" because the whole point of this tidbit of advice is that the math behind a should be specific and tailored to the problem you're solving. So while "do actual math" is broadly applicable advice, examples of what constitute actual math are bespoke and idiosyncratic.

I'm writing this blog post because it is a great example of what doing actual math in a machine learning context means. It's far from the only example in my career of using creative "actual math" to investigate a problem. But I hope it serves as a great.

**Also, shameless plug:** if you'd like to hire me to do some of this stuff to solve your company's problems, reach out!

## A seemingly innocuous model

The story of our model starts off strong, with _some_ amount of actual math.

The data scientists at this e-commerce company I consulted for were building pricing algorithms that provided differential pricing based on which user was looking at the product.

The basic idea behind the algorithm was to maximize profit by adjusting the price, and accounting for heterogeneity in user preferences:

$$\tilde{\pi} = \textrm{argmax}_{p} \big( A_i(p) * (p - C) \big)$$

Where:

- $\tilde{\pi}$ is estimated profit.
- $p$ is the price we set.
- $i$ is the index denoting the user-product combination.
- $C_i(\cdot)$ is the marginal cost function of a single sale for the user-product. Note that this product had uncertain costs that had to be estimated.
- $A_i(\cdot)$ is the user-product's "attach" function. Either a user purchases a product (i.e. they "attach"), or they do not. The function $A_i(\cdot)$ outputs a probability of attaching as a function of the price. (The assumption here is they only buy 1 or 0 quantity of the product.)

The so-called "attach" function was a logistic regression that took price and user attributes of both the product and the user:

$$A_i(p) = \textrm{logistic}\big(\beta_{p} p + \beta_0 + \beta_{x_{1}}x_{1,i} \ldots \beta_{x_{K}}x_{K,i} \big)$$

If you see any subtle oddities with this function's notation, note that **these are intentional representations on my part**: I'm representing how the model was actually implemented, not how it should have been. I'll get to a better representation of the problem shortly. (It should also be noted that these products were all within a similar product class, so for example, $\beta_p$ instead of $\beta_{p_i}$ isn't as facially ridiculous as it seems at first glance; they were not selling apples and oranges with the same pricing model. There **are** a lot of issues with this model--again, hold your horses--but it's not _that_ dramatic.)

To get out of math notation land and into real world land, you can just think of the data like this:

- 1 row = 1 impression for the product.
- There is an `attach` attribute, either 0 or 1, that denotes the dependent variable in a logistic regression.
- The other attributes of the data are mainly a denormalization of user attributes and product attributes, used as features.

And then, they would do something like this more or less (except in Tensorflow):

```python
attach_model = sm.Logit(
    endog=df["attach"],
    exog=df[["const", "price", "feature1", "feature2"]]
).fit_regularized(alpha=[0, 1, 1, 1])

cost_model = sm.OLS(
    endog=df["attach"],
    exog=df[["const", "price", "feature1", "feature2"]]
).fit_regularized(alpha=[0, 1, 1, 1])
```

And then, in production, loop over a range of prices, and apply the best one:

```python
est_profits = {}
for p in range(10, 50):
    row = generate_row(request_data, p)
    est_attach = attach_model.predict(row)
    est_cost = cost_model.predict(row)
    est_profits.loc[p] = est_attach * p - est_cost
return_price = max(est_profits, key=est_profits.get)
```

That's the basic idea, at least.

I do commend the data scientists in this case for thinking through the problem to start. Pricing is not a "prediction" problem, it's an _assignment_ problem. You can predict the conditional probability of a user purchasing something, but you can't "predict" the price.

There's just one issue: it doesn't really work.

## Going deeper into the math

Let's take a slightly different approach to the math to start unraveling the issues with the model.

First, note that we are just adjusting the price, and the other features the data scientists were using in this case were invariant in price. This means we can reduce the dot product of almost the entire coefficient and feature vector down to a single constant %b_i$ when we solve for the objective function:

$$ b_i = \beta_0 + \beta_{x_{1}}x_{1,i} \ldots \beta_{x_{K}}x_{K,i} $$

$$ \frac{\partial b}{\partial p} = 0 $$

We will also set the negative value of $\beta_p$ to equal $m_i$:

$$ m_i = -\beta_p $$

I will make it clear soon why we're doing this and why it's indexed to $i$. The only note for now that I will mention is that we slap a negative sign in front of $\beta_p$ because the slope in $p$ needs to be negative for the model to make any sense. So instead of always describing a negative slope, I instead opt to flip the sign so we can just work with a positive number.

The ultimate result of these transformations is that we can express a model with potentially hundreds of features as just being in 2-parameter space:

$$ A_i(p) = \textrm{logistic}(b_i - m_i p) $$

(This is the same function as before, just rewritten.)

Let's also do something similar for the cost function, using $c_i$ and $d_i$. (The cost function represents a prediction, so we don't include the residual):

$$ C_i(p) = c_i + a_i p $$

Second, note that $\textrm{logistic}(b_i - m_i p) = \frac{1}{1+\e^{m_i p - b_i}}$. Which means:

$$\tilde{\pi} = \textrm{argmax}_{p} \bigg( \frac{p - (c_i + a_i p)}{1+e^{m_i p - b_i}}\bigg)$$

$$\tilde{\pi} = \textrm{argmax}_{p} \bigg( \frac{ (1 - a_i) p - c_i}{1+e^{m_i p - b_i}}\bigg)$$

This is getting a little complex! It might be easier to first start with a **revenue-maximizing** model rather than a profit-maximizing model. So set $c_i=0$ and $a_i=0$:

$$\tilde{\textrm{Rev}} = \textrm{argmax}_{p} \bigg( \frac{p}{1+e^{m_i p - b_i}}\bigg)$$

Why do all this? Because **now that we have a function defined, we can solve for the closed form solution.**

Let's start with the simpler revenue model. We don't need to actually do the math ourselves, that is what Wolfram Alpha is for. Remember how to do this?

- Take the first partial derivative with respect to $p$.
- Find its root.

And here's what Wolfram Alpha says:

$$ p = \frac{W(e^{b_i-1})+1}{m_i} $$

Where $W(\dot)$ is the Lambert W function.

So, without going any further, a few things come to mind:

- As long as we can do this with the cost function mixed in, and as long as we can algebraically transform the logistic regression to $b_i - m_i p$, we don't actually need to.
- When the slope $m_i$ doubles, the price halves.
- The numerator $W(e^{b-1})+1$ is a little wonky, but long story short, **for reasonably expected values of $b_i$, changes in $b_i$ don't impact the price of the product that much.

That last note is important: we can take an _additional_ partial derivative to do what you might call a "sensitivity analysis." But instead of brute forcing, you can just calculate it with math:

$$ \frac{\partial p}{\partial b_i} = \frac{1}{m_i} \bigg( \frac{W(e^{b_i-1})}{W(\exp(b_i-1))+1} \bigg)$$

$$ \frac{\partial p}{\partial m_i} = - \frac{W(\exp(b_i-1))+1}{m^2}$$

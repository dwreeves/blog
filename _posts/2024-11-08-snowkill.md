---
layout: post
title: "SnowKill and how to best leverage it"
image: /assets/2024-11-08/notification_example_1.png
description: A neat tool for query monitoring in Snowflake
date: 2024-11-08
---

![Example SnowKill notification](/blog/assets/2024-11-08/notification_example_1.png)

[SnowKill](https://github.com/littleK0i/SnowKill) is a really cool gem of a tool made by Vitaly Markov ([@littleK0i](https://github.com/littleK0i) on Github).
It had about 22 Github stars when I first found it, and sits at 30 as of writing.

I'm writing about SnowKill because I think it deserves a little more love. This is a very well-made project that is easy to set up and adds a lot of value to any company using Snowflake.

Basically, it monitors for queries in Snowflake that are taking an excessively long time to run, and also checks the query plans for things like cartesian joins and excessive memory consumption.
You can also configure the specifics of what queries and conditions trigger alerts, but the basic example provided in the docs does a pretty decent job out of the box.

Also, SnowKill is pretty conservative with warehouse resource utilization; it uses an API that doesn't spin up the warehouse, and only uses the warehouse when absolutely necessary (most notably for stateful operations).
At my current job, we've been running it for a few weeks and it's used less than 1 hour of credits on an XSMALL warehouse.
Not bad!
SnowKill has also helped identify a weird rare issue in our production system that we hadn't noticed before, so we've already gotten a little bit of milage out of it.

## Getting the most out of SnowKill

Below are some notes I'd like to add about SnowKill that aren't currently mentioned in [the SnowKill docs](https://docs.snowkill.net/).

### Deploying in Airflow and Snowflake

I think the biggest shortcoming of SnowKill is that the documentation makes it seem harder to deploy more than it actually is. The docs are very general, abstract, and try to be un-opinionated about deployment.

If you aren't comfortable deploying things, or your org is large enough such that you have to wade through tons of bureaucracy to get things deployed, then you may struggle with the docs. The docs encourage you to set up a cron job and a separate database instance such as Postgres to get things working.

In my opinion, I think the easiest way to deploy SnowKill is:

- Use whatever orchestration tool you're using internally, be that Airflow, Dagster, Prefect, etc.
- Use Snowflake (not Postgres) as the data store.

This dramatically simplifies the deployment. Basically, SnowKill just needs to run Python code on a schedule and then save the query IDs that get alerted on so that you don't get alerted multiple times about a single query. You can do all of that without spinning up additional infrastructure assuming you're already running a general purpose orchestrator.

For the Snowflake deployment, if you have hybrid tables available in your region, you should use a hybrid table.
That will look something like this:

```sql
create schema your_db_here.snowkill;
create hybrid table your_db_here.snowkill.snowkill_log
(
  query_id varchar(16777216),
  check_result_level number(38,0),
  check_result_name varchar(16777216),
  check_result_description varchar(16777216),
  check_result_time timestamp_ntz(6),
  constraint snowkill_log_pkey primary key (query_id, check_result_level)
);
```

Make sure to also grant ownership of the `snowkill` schema to whatever executing role you're using and all that jazz.

Even if you are in a region without hybrid tables, I would still recommend just using Snowflake; just create a regular table without a primary key. Keeping infrastructure simple for a tool like this is important!

For an Airflow deployment, I have below a code example that is approximately (not exactly) how we have deployed it internally.
For security purposes, we grant a separate role to the SnowKill execution from our typical Airflow Snowflake execution, which can be done via granting the SnowKill role to whatever your Airflow role is and then using that role in the `SnowflakeHook`.

```python
from contextlib import closing
from datetime import datetime
from functools import cached_property
from typing import TYPE_CHECKING

from airflow import DAG
from airflow.models.baseoperator import BaseOperator

if TYPE_CHECKING:
    from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook
    from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook
    from airflow.utils.context import Context


default_args = {
    "snowflake_conn_id": "snowflake_default",
    "slack_webhook_conn_id": "slack_webhook_default"
}


class RunSnowKillOperator(BaseOperator):

    def __init__(
            self,
            *,
            snowkill_target_table: str,
            snowsight_base_url: str,
            slack_webhook_conn_id: str | None = None,
            snowflake_conn_id: str | None = None,
            **kwargs,
    ) -> None:
        super().__init__(**kwargs)
        self.slack_webhook_conn_id = slack_webhook_conn_id
        self.snowflake_conn_id = snowflake_conn_id
        self.snowkill_target_table = snowkill_target_table
        self.snowsight_base_url = snowsight_base_url

    @cached_property
    def slack_hook(self) -> "SlackWebhookHook":
        from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook
        return SlackWebhookHook(slack_webhook_conn_id=self.slack_webhook_conn_id)

    @cached_property
    def snowflake_hook(self) -> "SnowflakeHook":
        from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook
        return SnowflakeHook(
            snowflake_conn_id=self.snowflake_conn_id,
            # database="UTILS",
            # schema="MONITOR",
            # role="ADMIN_MONITOR_ROLE",
            # warehouse="ADMIN_MONITOR_WH"
        )

    def execute(self, context: "Context") -> None:
        import snowkill as sk

        with closing(self.snowflake_hook.get_conn()) as conn:
            snowkill_engine = sk.SnowKillEngine(conn)
            snowkill_storage = sk.SnowflakeTableStorage(conn, self.snowkill_target_table)
            snowkill_formatter = sk.SlackFormatter(self.snowsight_base_url)

            checks = [
                sk.ExecuteDurationCondition(
                    warning_duration=60 * 30,  # 30 minutes for warning
                    kill_duration=60 * 60,  # 60 minutes for kill
                ),
                sk.CartesianJoinExplosionCondition(
                    min_output_rows=1_000_000,  # join emits at least 1M output rows
                    min_explosion_rate=5,  # ratio of output rows to input rows is at least 5x
                    warning_duration=60 * 5,  # 5 minutes for warning
                    kill_duration=60 * 10,  # 10 minutes for kill
                ),
                sk.JoinExplosionCondition(
                    min_output_rows=10_000_000,  # join emits at least 10M output rows
                    min_explosion_rate=10,  # ratio of output rows to input rows is at least 10x
                    warning_duration=60 * 10,  # 10 minutes for warning
                    kill_duration=60 * 20,  # 20 minutes for kill
                ),
                sk.UnionWithoutAllCondition(
                    min_input_rows=10_000_000,  # at least 10M input rows for UNION without ALL
                    notice_duration=60 * 10,  # 10 minutes for notice
                ),
                sk.StorageSpillingCondition(
                    min_local_spilling_gb=50,  # 50Gb spill to local storage
                    min_remote_spilling_gb=1,  # 1Gb spill to remote storage
                    warning_duration=60 * 10,  # 10 minutes for waring
                    kill_duration=60 * 20,  # 20 minutes for kill
                ),
                sk.QueuedDurationCondition(
                    notice_duration=60 * 30,  # query was in queue for 30 minutes
                ),
                sk.BlockedDurationCondition(
                    notice_duration=60 * 5,  # query was locked by another transaction for 5 minutes
                ),
                sk.EstimatedScanDurationCondition(
                    min_estimated_scan_duration=60 * 60 * 2,  # query scan is estimated to take longer than 2 hours
                    warning_duration=60 * 10,  # warning after 10 minutes
                    kill_duration=60 * 20,  # kill after 20 minutes
                ),
            ]

            check_results = snowkill_engine.check_and_kill_pending_queries(checks)
            self.log.info(f"[{len(check_results)}] queries matched check conditions")
            check_results = snowkill_storage.store_and_remove_duplicate(check_results)
            self.log.info(f"[{len(check_results)}] queries remained after store deduplication")

            # Send notification for each new check result
            for result in check_results:
                self.slack_hook.send(blocks=snowkill_formatter.format(result))


with DAG(
        dag_id="snowkill",
        description="SnowKill performs real time query monitoring in Snowflake.",
        schedule="*/10 * * * *",  # Every 10 minutes
        catchup=False,
        start_date=datetime(2024, 1, 1),  # Set an appropriate start date here
        default_args=default_args
) as dag:
    RunSnowKillOperator(
        task_id="run_snowkill",
        snowkill_target_table="UTILS.MONITOR.SNOWKILL_LOG",
        snowsight_base_url="https://app.snowflake.com/my-org/my-account",
        max_active_tis_per_dag=1
    )
```

### Enhancing security of your SnowKill deployment

On the topic of security, at my current workplace, we take security very seriously.
I have no reason to not trust the author of this library, but it's still best practice to adhere to the principle of least privilege.
(This is far from a matter of trusting the author of a library isn't a malicious actor, of course. For example, what if their Github account or PyPI account gets broken into? What if your own Airflow deployment gets broken into and there are `ACCOUNTADMIN` privileges lying around that otherwise wouldn't be there if not for SnowKill? What if you don't want other members of your org to have potential access to `ACCOUNTADMIN` through Airflow? Etc.)

And on that note, the docs are unclear on one thing.
Basically, the docs tell you that you need to `grant role ACCOUNTADMIN ...` to the user/role that SnowKill uses.
To the security conscious, this will be a point of concern. We would not have implemented SnowKill if this was a hard requirement.

**Thankfully, it is not 100% accurate you _need_ to `grant role ACCOUNTADMIN`,** as long as you don't mind missing out on one single SnowKill feature.

Basically, the `ACCOUNTADMIN` role is the least privileged access only for the operation `SHOW LOCKS IN ACCOUNT;` which is only used by the condition `BlockedDurationCondition()`.
(This is unfortunate, and Snowflake should decouple this permission from the `ACCOUNTADMIN` role, but it is what it is for now.)
For _every other condition_ that SnowKill checks, I believe all you need is:

```sql
GRANT MANAGE WAREHOUSES ON ACCOUNT TO ROLE X;
```

This still gives complete access to SnowKill of the text of your SQL queries, but hides the actual data being operated on.
If you'd like to hide specific warehouses from SnowKill, then you will need to `grant monitor,operate on warehouse X to role Y;` for each individual warehouse you'd like SnowKill to be able to look at.

This does mean you cannot use `BlockedDurationCondition()`, but for non-transactional workflows, this is not a big deal.
I think it's a tough sell to get people to `GRANT ROLE ACCOUNTADMIN` to a library with 30 Github stars, or just to put that permission into an Airflow/Dagster/etc. runtime in general, so I think granting `MANAGE WAREHOUSES ON ACCOUNT` should hopefully bridge the gap between your desire to use this tool and security concerns.

## Disclaimer

The views expressed here are my own and do not represent the views of my employer;
furthermore, there is no investment advice intended.
